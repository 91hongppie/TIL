# 선형회귀

- 변수 사이의 선형적인 관계를 모델링 한 것
- 선형적이다? 
  - 직선적이다.
- 선형적이다! 
  - 선형적인 관계에 적용하는 대표적인 기계 학습 이론이 선형 회귀이다
- 선형 회귀 모델을 구축한다
  - 주어진 데이터를 학습시켜서 가장 합리적인 '직선'을 찾아내는 것
  - 따라서 데이터는 3개 이상일 때 의미가 있다.
- H(x) = Wx + b
  - 하나의 방정식을 이용해 직선을 표현한다.
  - 가설을 수정해 나가면서 가장 합리적인 식을 찾아낸다. -> 선형 회귀의 핵심
- 즉, 선형 회귀란 주어진 데이터를 이용해 일차방정식을 수정해나가는것!
  - 학습을 거쳐서 가장 합리적인 선을 찾아내는 것
  - 학습을 많이 해도 **완벽한** 식을 찾아내지 못할수있다.
  - 하지만 실제 사례에서는 근사값을 찾는 것 만으로도 충분할 때가 많다.
  - 알파고도 결과적으로는 **근사값**을 가정하는 프로그램에 불과

### 직관적으로 이해하기

![시간당 매출](C:\Users\multicampus\Desktop\시간당 매출.PNG)

- 처음에 데이터와 상당히 거리가 먼 상태로 직선을 그렸을 때
- 데이터와 비교하여 수정하면서 합리적인 직선을 발견한다.
- 점점 변화의 폭이 작아진다.

## 비용(Cost)

- 비용 : 가설이 얼마나 정확한 지 판단하는 기준

  ![비용](C:\Users\multicampus\Desktop\비용.PNG)

- 직선과의 거리를 구하여 비용을 계산할수있다

- 비용함수(Cost Function)
  $$
  (예측 값) - (실제 값)^2 의 평균
  $$

  - 현재의 W, b 값과 데이터를 이용하면 비용 함수를 구할 수 있습니다.

  - 비용 함수로 구한 비용이 적을수록 좋습니다.

    - $$
      cost(W,b) = \frac{1}{n}\sum_{m}^{i=1}(H(x^{(i)})=y^{(i)})^2
      $$

      

## 경사 하강(Gradient Descent)

- 경사 하강을 이용해 합리적인 식을 도출합니다.

  - H(x) = Wx 로 식을 간단히 합니다.

  - 따라서 비용 함수는 
    $$
    (Wx - y)^2
    $$
    

    를 따릅니다.

  - ![경사](C:\Users\multicampus\Desktop\경사.PNG)

  - 가장 깊은 골짜기에 도달할 때까지 경사를 타고 내려간다.

  - 경사를 타고 내려가면서 미분을 통해 기울기를 구한다. 

  - 가장 깊은 골짜기는 기울기가 0(수평) -> 수평을 이룰수록 좋은 식을 찾는 것이 선형회귀를 적용한 머신러닝을 구현할 때 경사하강법을 이용한 가장 핵심적인 개념

  - 합리적인 직선을 찾는 과정

## 핵심

### 점프! 얼마나 뛰어야 하나?

- 곡선의 특성상 초반에 많은 폭으로 변화합니다.
- 너무 작게 점프하면 오랫동안 학습해야 합니다.
- 너무 크게 점프하면 학습 결과가 부정확할 수 있습니다.
  - ![크게 점프](C:\Users\multicampus\Desktop\크게 점프.PNG)
  - 학습의 결과가 부정확한 식을 도출할수있습니다.

### 텐서플로우는 경사 하강 라이브러리를 제공한다.

- 우리는 직접 구현할 필요 없이 원리만 이해해도 충분

# 선형 회귀 구현하기

