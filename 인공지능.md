# 인공지능

머신러닝 : 빅데이터를 해석하는데 가장 많이 쓰이는 방법, 비정형데이터 분석가능

데이터 마이닝: 정형 데이터(사람의 나이, 성별, 거주지역이 테이블에 나타난 것)

비정형 데이터 : 이미지 데이터, 텍스트 데이터(뉴스기사, 블로그포스트, ...) 

머신러닝은 AI의 일부분이라고 할 수 있다. 데이터를 통계적으로 이용해서 만드는 방법

머신러닝과 통계학과 밀접한 관계가 있다. 통계학을 가져다 사용하는게 머신러닝이라고 할 수 있다.

머신러닝에서 기본적으로 다루는 문제들 

- 지도학습 - 정답이 있는 데이터를 트레이닝을 통해 미리 가지고 있다. 그걸 기반으로 다른 사진을 분류
  - 분류 - 선형모델, 비선형 모델
    - 비선형 모델
      1. 데이터에 대해서 계속 질문을 하는 모델
    - 선형 모델
      1. 
- 비지도학습 
  - 몇개의 그룹으로 나눌 것이냐 - 데이터의 생김새에 따라 달라진다.
  - K-means clustering
    - 기준을 찾고 거리에 따라 분류
  - DB Scan
    - 임의의 데이터에서 시작해서 특정 거리 안에있는 데이터를 한 그룹으로 분류
- 강화학습 - 오늘은 안함
- Representation Learning - 딥러닝과 관련이 있다.



## Famous AI Systems

- IBM WATSON
- 알파고
- 알파고 제로
- Google Duplex
  - 미용실, 호텔, ... 에 직접 전화를 걸어서 대화를하며 예약을 해주는 서비스

## AI Intelligence

- Visual Intelligence
  - MNIST(data set)
    - 숫자 필기 인식
  - ImageNet
    - 수천개의 클래스의 이미지들이 있다.
    - 클래스는 - 포유류 -> 강아지 -> 허스키, 운송수단 -> 비행기, 배 -> 배 
- Language Intelligence
  - 정보를 주고 정보에 대해 질문을 하고 그 질문에 대해 답을 할 수 있는지

AI 의 기본 머신러닝, 딥러닝

AI의 역사





## Linear Regression

- 지도학습에 해당
- 주어진 x 에 대해서 y 값을 실수로 예측하는 것 (예) 강남구의 집 값을 예측할 때 사용할 수 있다. 어떤 손님이 매장에서 얼마를 쓸 것이냐 예측
- Regression 에서 가장 간단한 방법
- x 값이 주어졌을 때 y 값을 예측하도록 라인이 주어진다.
  - 이유 - 라인을 보고 x 에 대해 y 값을 예측

polymonial Regression

x의 지수가 커질수록 정확해진다.

Multivariate linear regression

RSS - 예측된 y 값과 실제 y 값의 차이를 줄이는 w를 구한다

Ridge Regression

overfit - 쓸데없이 복잡한 커브가 나왔다.

regularization - 복잡한건 싫다. 심플한게 좋다.

데이터가 많으면 모델이 자연스럽게 regularization 된다. - 빅데이터가 가지고 있는 힘

빅데이터의 regularization 효과

머신러닝 모델은 트레이닝 데이터에 대해서는 학습효과가 뛰어나야한다.

truth 와 model의 degree 가 일치해야 에러값이 적게 나타나고 좋다. 데이터가 많으면 에러가 더 적어진다. 하지만 truth 와 model 의 degree 의 차이가 너무 크면 데이터가 많더라도 에러가 있다.

overfit - 트레이닝 데이터와 텍스트 데이터의 차이가 얼마나 나는지





## Naive Bayes Classifier(나이브 베이즈) - 모델

- 나이브 베이즈 모형은 모형 자체가 말이 되는 모형
- 이게 왜 이렇게 분류 됐을까 에 대해 직접적으로 접근하는 모델
- 굉장히 많은 머신러닝 연구에서 나이브 베이즈를 기본 모형으로 쓴다.
- 나이브 베이즈는 baseline 같은 것

Features

- 성질
- 데이터에 대한 세부사항
- 우리가 분류하고자하는 데이터에 대해서 숫자로 표현해주는 과정

여러가지 확률을 계산해서 

확률은 트레이닝 데이터에서 나온다

### 단어의 빈도수

- bag on words
  - 단어가 몇번 나왔나

Overfitting

여기서도 좋지않다.

### Smoothing

- 확률 분자 분모에 + 1 씩 해주는 것  - 라플라스 스무딩
- 조금 더 잘하면 k / 1000 + k
- 이러면 overfitting 이 일어나지 않는다.
- 이렇게 하면 나이브베이즈가 좀 더 정확해진다
- Hyperparameters
  - k, alpha
    - held-out data = validation data  - lambda, k 에 가장 적합한 값을 찾기위해 사용
    - training - CPT, W 를 학습하기 위해 사용하는 데이터
    - test data - 학습한 모델이 얼마나 좋은지 정확도는 test data에 대해서 한다.
  - 1000개의 데이터
    - 800개는 training data
    - 100개는 validation data
    - 100개는 test data

### Baselines

실제 연구에서는 나이브베이즈보다 강력한 모델을 사용

### 에러가 생기는 경우

스팸메일이 남아있는 경우 

features 를 더 많이 써야한다. - 단어만으로는 부족하다

보낸 사람이나 다른 사항들을 봐야한다

### Features

- 확률값을 가지고 있는 
- 대부분의 classifiers 는 픽셀에 대해 0이냐 1이 아니라 정확한 값을 넣어서 확률을 구한다.

- 데이터에 대해서 숫자로 만들수 있는 데이터의 다양한 성질

## Summary

나이브 베이즈 - 조건부 확률이 나오기 때문에 사용

중요한 assumption은 features 들이 독립적이다 

실제 시스템에서 새로운 데이터에 대해 잘 예측할수있게 하려면 smoothing 은 중요하다.



## Deep Natural Language Processing(NLP)

### Word, Sentence, and Document Embedding

1. Word Vector Representation : 워드 임베딩

   1. 단어 하나하나에 대해 벡터로 표현하는것

      이유: 데이터 셋에서 워드 벡터로 학습을 하면 관계(Semantic, syntactic)가 표현되고 

   Distribution Hypothesis

   - word vector 에서 유사한 단어들은 가까운 곳에 있어야한다.

   CBOW

   - 문맥을보고 중간에 어떤 단어가 들어올지 예측

   Skip-Gram

   - 중간의 단어를 알 때 양 옆에 어떤 단어가 나올수있는지 예측

Word Similarity Task

- 단어의 유사도 측정 (소년, 청년 = 8.83, 식품,수탉 = 4.42)
- 유사도가 높을수록 그래프에서 각도의 차이가 작아서 cosine similarity 가 높다.

Word Analogy Task

- 두 단어의 관계에서 다른 하나의 단어와 같은 관계에 있는 것을 찾는것

Subword information Skip-Gram(a.k.a FastText)

단어를 쪼개서 모델링한다 (예)where -> wh, whe, her, ere, re, where

NLP Trend: Transfer Learning from Language Models

워드 임베딩의 문제 1 - 컨텍스트가 없다. 양 옆의 단어에 상관없이 단어가 같으면 같은 임베딩

elmo에서는 컨텍스트 기반이기 때문에 양 옆의 단어를 고려해서 다른 임베딩을 갖게한다.

### 뉴럴넷 - 인풋이 있고 히든레이어가 있고 아웃풋이 있다. 인풋은 단어 하나하나, 이미지 픽셀. 히든레이어는 인풋 * rate 의 합 이 히든레이어에서 논리니어 펑션이나 다양한것을 쓰면 아웃풋으로 

### ELMO 의 포인트

- 단어가 가지고 있는 의미를 잘 캡쳐하고 단어가 어떤 문맥에서 나왔는지 고려해서 의미를 캡쳐하는 모델을 만든다
- 문제 
  1. 긴 시퀀스에 대해서는 잘 학습을 하지 못한다.
     1. Transfomers : 단어가 멀리 있어도 연관이 있는것을 캡쳐하는 것

### BERT

- 단어에 대해서 모델링 할 때 양방향으로 모델링 하는 것
- 두 개의 sentence가 주어졌을 때 어떤 관계인지
- 한 개의 sentence가 긍정적이냐 부정적이냐
- 질문에 답하는 Task



